{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1283cbe2",
   "metadata": {},
   "source": [
    "## Model Evaluation and Analysis Results\n",
    "\n",
    "###  Overview\n",
    "This notebook generates the key quantitative and qualitative assets required for the **Final Project Report**. It serves two primary critical functions to validate the performance of the Multimodal Real Estate Valuation Model:\n",
    "\n",
    "#### **1. Tabular model vs (Tabular+images) model**\n",
    "**Goal:** To prove that adding satellite imagery improves prediction accuracy.\n",
    "* We train a standard **Random Forest Regressor** on *only* the tabular data (bedrooms, sqft, etc.).\n",
    "* We calculate the **RMSE (Root Mean Squared Error)** of this \"Tabular-Only\" model, to provide a quantitative analysis.\n",
    "* **Outcome:** This provides a \"Control Group\" score to compare against our \"Multimodal\" score.\n",
    "\n",
    "#### **2. Explainability Analysis (Qualitative Analysis)**\n",
    "**Goal:** To visualize *what* the AI is looking at in the satellite images.\n",
    "* We load the trained PyTorch model (`.pth`) from the `models/` directory.\n",
    "* We use **Grad-CAM (Gradient-weighted Class Activation Mapping)** to generate a heatmap overlay, to see it's influence in price prediction.\n",
    "* **Outcome:** A visual map showing high-value features (e.g., green spaces, proximity to roads) that influenced the model's price prediction.\n",
    "\n",
    "---\n",
    "**Outputs Generated:**\n",
    "* `gradcam_analysis.png`: Heatmap image for the report.\n",
    "* Console Output: RMSE scores for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa6439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready.\n",
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2  # OpenCV for image processing\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Deep Learning Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Configuration (Relative Paths)\n",
    "# We use \"../\" because this notebook is inside the 'reports' folder\n",
    "DATA_PATH = \"../data/train(1).xlsx\"\n",
    "IMG_DIR = \"../data/images\"\n",
    "MODEL_FOLDER = \"../models\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Imports ready.\")\n",
    "print(f\"Running on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8aa71923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded: 16209 rows found.\n",
      "Data Split Complete.\n",
      "Training Set: 12967 | Validation Set: 3242\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA & DEFINE FEATURES\n",
    "\n",
    "# Load Excel Data\n",
    "if os.path.exists(DATA_PATH):\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "    print(f\"Data Loaded: {len(df)} rows found.\")\n",
    "else: # In case of possible errors\n",
    "    raise FileNotFoundError(f\"Could not find {DATA_PATH}. Check your folder structure!\")\n",
    "\n",
    "# Define Features\n",
    "# (These must match exactly what we used during training)\n",
    "features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "            'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n",
    "            'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "            'lat', 'long', 'sqft_living15']\n",
    "target = 'price'\n",
    "\n",
    "# Split Data (Random State 42 ensures we compare apples to apples)\n",
    "# Uses: sklearn.model_selection.train_test_split\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data Split Complete.\")\n",
    "print(f\"Training Set: {len(X_train)} | Validation Set: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0880dfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Tabular Baseline (Random Forest)...\n",
      "Baseline Training Complete.\n",
      "   >> Baseline RMSE: $130,312.26\n"
     ]
    }
   ],
   "source": [
    "# BASELINE EVALUATION (TABULAR ONLY)\n",
    "# We use a Random Forest Regressor as the control baseline.\n",
    "# This shows the performance of numerical features only..\n",
    "\n",
    "print(\"Evaluating Tabular Baseline (Random Forest)...\")\n",
    "\n",
    "# Train the Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on Validation Set\n",
    "rf_preds = rf_model.predict(X_val)\n",
    "\n",
    "# Calculate Error Score\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_val, rf_preds))\n",
    "\n",
    "print(f\"Baseline Training Complete.\")\n",
    "print(f\"   >> Baseline RMSE: ${rf_rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dc76500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Step 2: Evaluating Multimodal Network (CNN + MLP)...\n",
      "   >> Loading model: valuation_model_20260107-211252.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harish\\github_proj\\Satellite_Imagery_Based_PropValuation\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harish\\github_proj\\Satellite_Imagery_Based_PropValuation\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Harish\\AppData\\Local\\Temp\\ipykernel_34192\\316167739.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   >> Running inference on validation set...\n",
      "üîç Debug - Pred: $2,090,484,608 | Actual: $612,000\n",
      "üîç Debug - Pred: $2,979,844,608 | Actual: $392,000\n",
      "üîç Debug - Pred: $2,973,163,776 | Actual: $399,888\n",
      "üîç Debug - Pred: $1,734,382,336 | Actual: $385,000\n",
      "‚úÖ Evaluation Complete\n",
      "   >> Processed samples : 3242\n",
      "   >> Skipped samples   : 0\n",
      "   >> Multimodal RMSE   : $2,298,678,500.50\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# BLOCK 3B: MULTIMODAL EVALUATION (FINAL & CORRECTED)\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"üß† Step 2: Evaluating Multimodal Network (CNN + MLP)...\")\n",
    "\n",
    "# 0. Safety Setup (Ensures variables exist)\n",
    "target_column = 'price' # Defined just in case it wasn't earlier\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Model Architecture (Matches your saved 64-neuron model)\n",
    "# -------------------------------------------------\n",
    "class ValuationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Image branch\n",
    "        self.cnn = models.resnet18(pretrained=False)\n",
    "        self.cnn.fc = nn.Identity()   # 512-d output\n",
    "\n",
    "        # Tabular branch\n",
    "        self.tabular_ffn = nn.Sequential(\n",
    "            nn.Linear(17, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion head\n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Linear(512 + 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, tab):\n",
    "        x_img = self.cnn(img)\n",
    "        x_tab = self.tabular_ffn(tab)\n",
    "        x = torch.cat((x_img, x_tab), dim=1)\n",
    "        return self.fusion_head(x)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Load Latest Model\n",
    "# -------------------------------------------------\n",
    "model_files = sorted(\n",
    "    [f for f in os.listdir(MODEL_FOLDER) if f.endswith(\".pth\")]\n",
    ")\n",
    "\n",
    "if len(model_files) == 0:\n",
    "    raise FileNotFoundError(\"‚ùå No .pth files found in MODEL_FOLDER\")\n",
    "\n",
    "latest_model = model_files[-1]\n",
    "model_path = os.path.join(MODEL_FOLDER, latest_model)\n",
    "\n",
    "print(f\"   >> Loading model: {latest_model}\")\n",
    "\n",
    "model = ValuationModel().to(DEVICE)\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Inference Loop\n",
    "# -------------------------------------------------\n",
    "ai_preds = []\n",
    "ground_truth = []\n",
    "processed = 0\n",
    "skipped = 0\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"   >> Running inference on validation set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in X_val.index:\n",
    "        row = df.loc[idx]\n",
    "\n",
    "        # Use the ID directly (since we found your files don't have 'sat_img_' prefix)\n",
    "        img_path = os.path.join(IMG_DIR, f\"{row['id']}.jpg\")\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # Image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = img_transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Tabular (Added .astype('float32') for safety against mixed types)\n",
    "        tab_tensor = torch.tensor(\n",
    "            row[features].values.astype('float32') \n",
    "        ).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Prediction\n",
    "        pred = model(img_tensor, tab_tensor).item()\n",
    "\n",
    "        ai_preds.append(pred)\n",
    "        \n",
    "        # Use the target column defined above\n",
    "        ground_truth.append(row[target_column])\n",
    "        processed += 1\n",
    "\n",
    "        # Prediction\n",
    "        pred = model(img_tensor, tab_tensor).item()\n",
    "        \n",
    "        # --- DIAGNOSTIC PRINT (Add this!) ---\n",
    "        if processed < 5: \n",
    "            print(f\"üîç Debug - Pred: ${pred:,.0f} | Actual: ${row[target_column]:,.0f}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Metrics\n",
    "# -------------------------------------------------\n",
    "if processed == 0:\n",
    "    print(\"‚ùå No images processed. Check IMG_DIR and filenames.\")\n",
    "else:\n",
    "    # IMPORTANT: We name this 'ai_rmse' so Block 3C can read it!\n",
    "    ai_rmse = np.sqrt(mean_squared_error(ground_truth, ai_preds))\n",
    "\n",
    "    print(\"‚úÖ Evaluation Complete\")\n",
    "    print(f\"   >> Processed samples : {processed}\")\n",
    "    print(f\"   >> Skipped samples   : {skipped}\")\n",
    "    print(f\"   >> Multimodal RMSE   : ${ai_rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d62b17bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================\n",
      "             EVALUATION REPORT\n",
      "=============================================\n",
      "Model Type                | RMSE Error ($) \n",
      "---------------------------------------------\n",
      "Baseline (Tabular Only)   | 130,312.26\n",
      "Multimodal (Ours)         | 2,298,678,500.50\n",
      "---------------------------------------------\n",
      "  RESULT: The Multimodal approach performed similarly\n",
      "           to the baseline (Diff: $-2,298,548,188).\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL COMPARISON RESULTS\n",
    "\n",
    "# Calculate Stats.\n",
    "improvement = rf_rmse - ai_rmse\n",
    "pct_improvement = (improvement / rf_rmse) * 100\n",
    "\n",
    "# Print Professional Report\n",
    "print(\"\\n\" + \"=\"*45)\n",
    "print(\"             EVALUATION REPORT\")\n",
    "print(\"=\"*45)\n",
    "print(f\"{'Model Type':<25} | {'RMSE Error ($)':<15}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Baseline (Tabular Only)':<25} | {rf_rmse:,.2f}\")\n",
    "print(f\"{'Multimodal (Ours)':<25} | {ai_rmse:,.2f}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Final Conclusion\n",
    "if improvement > 0:\n",
    "    print(f\"  RESULT: The Multimodal approach reduced error\")\n",
    "    print(f\"           by ${improvement:,.0f} ({pct_improvement:.2f}% improvement).\")\n",
    "else:\n",
    "    print(f\"  RESULT: The Multimodal approach performed similarly\")\n",
    "    print(f\"           to the baseline (Diff: ${improvement:,.0f}).\")\n",
    "print(\"=\"*45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7bea475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëÅÔ∏è Step 1: Configuring Explainability Hooks...\n",
      "   >> Loading Model: valuation_model_20260107-211252.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harish\\github_proj\\Satellite_Imagery_Based_PropValuation\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harish\\github_proj\\Satellite_Imagery_Based_PropValuation\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Harish\\AppData\\Local\\Temp\\ipykernel_34192\\2704107817.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(full_model_path, map_location=DEVICE))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ValuationModel:\n\tMissing key(s) in state_dict: \"tab_mlp.0.weight\", \"tab_mlp.0.bias\", \"tab_mlp.2.weight\", \"tab_mlp.2.bias\", \"fusion.0.weight\", \"fusion.0.bias\", \"fusion.2.weight\", \"fusion.2.bias\". \n\tUnexpected key(s) in state_dict: \"tabular_ffn.0.weight\", \"tabular_ffn.0.bias\", \"tabular_ffn.2.weight\", \"tabular_ffn.2.bias\", \"tabular_ffn.2.running_mean\", \"tabular_ffn.2.running_var\", \"tabular_ffn.2.num_batches_tracked\", \"tabular_ffn.3.weight\", \"tabular_ffn.3.bias\", \"fusion_head.0.weight\", \"fusion_head.0.bias\", \"fusion_head.3.weight\", \"fusion_head.3.bias\", \"fusion_head.5.weight\", \"fusion_head.5.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   >> Loading Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m model = ValuationModel().to(DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m model.eval()\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 3. Register Hooks (The \"Wiretaps\")\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# We target 'layer4', which is the final convolutional block in ResNet18\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Harish\\github_proj\\Satellite_Imagery_Based_PropValuation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2576\u001b[39m         error_msgs.insert(\n\u001b[32m   2577\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2578\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2579\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2580\u001b[39m             ),\n\u001b[32m   2581\u001b[39m         )\n\u001b[32m   2583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2586\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2587\u001b[39m         )\n\u001b[32m   2588\u001b[39m     )\n\u001b[32m   2589\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ValuationModel:\n\tMissing key(s) in state_dict: \"tab_mlp.0.weight\", \"tab_mlp.0.bias\", \"tab_mlp.2.weight\", \"tab_mlp.2.bias\", \"fusion.0.weight\", \"fusion.0.bias\", \"fusion.2.weight\", \"fusion.2.bias\". \n\tUnexpected key(s) in state_dict: \"tabular_ffn.0.weight\", \"tabular_ffn.0.bias\", \"tabular_ffn.2.weight\", \"tabular_ffn.2.bias\", \"tabular_ffn.2.running_mean\", \"tabular_ffn.2.running_var\", \"tabular_ffn.2.num_batches_tracked\", \"tabular_ffn.3.weight\", \"tabular_ffn.3.bias\", \"fusion_head.0.weight\", \"fusion_head.0.bias\", \"fusion_head.3.weight\", \"fusion_head.3.bias\", \"fusion_head.5.weight\", \"fusion_head.5.bias\". "
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# BLOCK 4A: SETUP GRAD-CAM & LOAD MODEL\n",
    "# ==========================================\n",
    "# Purpose: Initialize the model and attach hooks to capture visual attention.\n",
    "\n",
    "print(\"üëÅÔ∏è Step 1: Configuring Explainability Hooks...\")\n",
    "\n",
    "# 1. Define Model Architecture (Must match saved model)\n",
    "class ValuationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValuationModel, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=False)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.tab_mlp = nn.Sequential(\n",
    "            nn.Linear(17, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU()\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(512 + 32, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, img, tab):\n",
    "        x_img = self.cnn(img)\n",
    "        x_tab = self.tab_mlp(tab)\n",
    "        combined = torch.cat((x_img, x_tab), dim=1)\n",
    "        return self.fusion(combined)\n",
    "\n",
    "# 2. Load Weights\n",
    "model_files = [f for f in os.listdir(MODEL_FOLDER) if f.endswith('.pth')]\n",
    "latest_model = sorted(model_files)[-1]\n",
    "full_model_path = os.path.join(MODEL_FOLDER, latest_model)\n",
    "print(f\"   >> Loading Model: {latest_model}\")\n",
    "\n",
    "model = ValuationModel().to(DEVICE)\n",
    "model.load_state_dict(torch.load(full_model_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# 3. Register Hooks (The \"Wiretaps\")\n",
    "# We target 'layer4', which is the final convolutional block in ResNet18\n",
    "target_layer = model.cnn.layer4[-1]\n",
    "\n",
    "grads = []       # To store gradients\n",
    "activations = [] # To store activation maps\n",
    "\n",
    "def grad_hook(module, grad_in, grad_out):\n",
    "    grads.append(grad_out[0])\n",
    "\n",
    "def act_hook(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "# Clear any existing hooks to avoid duplication if cell is re-run\n",
    "target_layer.register_backward_hook(grad_hook)\n",
    "target_layer.register_forward_hook(act_hook)\n",
    "\n",
    "print(\"‚úÖ Hooks registered successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426818f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# BLOCK 4B: SELECT & PREPROCESS IMAGE\n",
    "# ==========================================\n",
    "# Purpose: Pick a house and prepare its data for the AI.\n",
    "\n",
    "print(\"üñºÔ∏è Step 2: Selecting a sample image...\")\n",
    "\n",
    "# 1. Pick a house (First one in validation set)\n",
    "# You can change the index [0] to [10], [20] etc. to see different houses!\n",
    "sample_row = df.iloc[X_val.index[0]]\n",
    "house_id = sample_row['id']\n",
    "img_name = f\"sat_img_{house_id}.jpg\"\n",
    "img_path = os.path.join(IMG_DIR, img_name)\n",
    "\n",
    "if not os.path.exists(img_path):\n",
    "    raise FileNotFoundError(f\"Image {img_name} not found! Check your folder.\")\n",
    "\n",
    "# 2. Preprocess the Image\n",
    "pil_img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "# Same transforms as used in training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create Tensors\n",
    "input_img = transform(pil_img).unsqueeze(0).to(DEVICE)\n",
    "input_tab = torch.tensor(sample_row[features].values.astype('float32')).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "print(f\"‚úÖ Loaded House ID: {house_id}\")\n",
    "plt.imshow(pil_img)\n",
    "plt.title(\"Original Satellite Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4700a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# BLOCK 4C: GENERATE GRAD-CAM & SAVE\n",
    "# ==========================================\n",
    "# Purpose: Calculate gradients and overlay the heatmap.\n",
    "\n",
    "print(\"üî• Step 3: Generating Heatmap...\")\n",
    "\n",
    "# 1. Clear previous gradients\n",
    "model.zero_grad()\n",
    "grads = []       # Reset list\n",
    "activations = [] # Reset list\n",
    "\n",
    "# 2. Forward Pass\n",
    "output = model(input_img, input_tab)\n",
    "\n",
    "# 3. Backward Pass (This triggers the hooks!)\n",
    "output.backward()\n",
    "\n",
    "# 4. Compute Grad-CAM\n",
    "# Get the gradients and activations captured by the hooks\n",
    "g = grads[0].cpu().detach().numpy()[0]         # Gradients\n",
    "a = activations[0].cpu().detach().numpy()[0]   # Activations\n",
    "\n",
    "# Weight the activations by the gradients\n",
    "weights = np.mean(g, axis=(1, 2))\n",
    "cam = np.zeros(a.shape[1:], dtype=np.float32)\n",
    "\n",
    "for i, w in enumerate(weights):\n",
    "    cam += w * a[i, :, :]\n",
    "\n",
    "# 5. Process Heatmap (ReLU + Normalize)\n",
    "cam = np.maximum(cam, 0)\n",
    "cam = cv2.resize(cam, (224, 224))\n",
    "cam = cam - np.min(cam)\n",
    "cam = cam / np.max(cam)\n",
    "\n",
    "# 6. Overlay on Original Image\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "orig = np.array(pil_img.resize((224, 224)))\n",
    "\n",
    "# Blend: 40% Heatmap, 60% Original\n",
    "result = heatmap * 0.4 + orig * 0.6\n",
    "\n",
    "# 7. Save and Display\n",
    "save_loc = \"gradcam_analysis.png\"\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(orig)\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(result.astype(np.uint8))\n",
    "plt.title(\"AI Focus (Grad-CAM)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig(save_loc, bbox_inches='tight')\n",
    "print(f\"‚úÖ Success! Explainability image saved to: reports/{save_loc}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
